{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rover Project Test Notebook\n",
    "This notebook contains the functions from the lesson and provides the scaffolding you need to test out your mapping methods.  The steps you need to complete in this notebook for the project are the following:\n",
    "\n",
    "* First just run each of the cells in the notebook, examine the code and the results of each.\n",
    "* Run the simulator in \"Training Mode\" and record some data. Note: the simulator may crash if you try to record a large (longer than a few minutes) dataset, but you don't need a ton of data, just some example images to work with.   \n",
    "* Change the data directory path (2 cells below) to be the directory where you saved data\n",
    "* Test out the functions provided on your data\n",
    "* Write new functions (or modify existing ones) to report and map out detections of obstacles and rock samples (yellow rocks)\n",
    "* Populate the `process_image()` function with the appropriate steps/functions to go from a raw image to a worldmap.\n",
    "* Run the cell that calls `process_image()` using `moviepy` functions to create video output\n",
    "* Once you have mapping working, move on to modifying `perception.py` and `decision.py` to allow your rover to navigate and map in autonomous mode!\n",
    "\n",
    "**Note: If, at any point, you encounter frozen display windows or other confounding issues, you can always start again with a clean slate by going to the \"Kernel\" menu above and selecting \"Restart & Clear Output\".**\n",
    "\n",
    "**Run the next cell to get code highlighting in the markdown cells.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<style> code {background-color : orange !important;} </style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#%matplotlib qt # Choose %matplotlib qt to plot to an interactive window (note it may show up behind your browser)\n",
    "# Make some of the relevant imports\n",
    "import cv2 # OpenCV for perspective transform\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc as misc # For saving images as needed\n",
    "import glob  # For reading in a list of images from a folder\n",
    "import imageio\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "imageio.plugins.ffmpeg.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Look at the Data\n",
    "There's some example data provided in the `test_dataset` folder.  This basic dataset is enough to get you up and running but if you want to hone your methods more carefully you should record some data of your own to sample various scenarios in the simulator.  \n",
    "\n",
    "Next, read in and display a random image from the `test_dataset` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "outputExpanded": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = '../test_dataset/IMG/*'\n",
    "img_list = glob.glob(path)\n",
    "# Grab a random image and display it\n",
    "idx = np.random.randint(0, len(img_list)-1)\n",
    "image = mpimg.imread(img_list[idx])\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Data\n",
    "Read in and display example grid and rock sample calibration images.  You'll use the grid for perspective transform and the rock image for creating a new color selection that identifies these samples of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the simulator you can toggle on a grid on the ground for calibration\n",
    "# You can also toggle on the rock samples with the 0 (zero) key.  \n",
    "# Here's an example of the grid and one of the rocks\n",
    "example_grid = '../calibration_images/example_grid1.jpg'\n",
    "example_rock = '../calibration_images/example_rock1.jpg'\n",
    "grid_img = mpimg.imread(example_grid)\n",
    "rock_img = mpimg.imread(example_rock)\n",
    "\n",
    "fig = plt.figure(figsize=(12,3))\n",
    "plt.subplot(121)\n",
    "plt.imshow(grid_img)\n",
    "plt.subplot(122)\n",
    "plt.imshow(rock_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perspective Transform\n",
    "\n",
    "Define the perspective transform function from the lesson and test it on an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to perform a perspective transform\n",
    "# I've used the example grid image above to choose source points for the\n",
    "# grid cell in front of the rover (each grid cell is 1 square meter in the sim)\n",
    "# Define a function to perform a perspective transform\n",
    "def perspect_transform(img, src, dst):\n",
    "           \n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    warped = cv2.warpPerspective(img, M, (img.shape[1], img.shape[0]))# keep same size as input image\n",
    "    \n",
    "    return warped\n",
    "\n",
    "# Define calibration box in source (actual) and destination (desired) coordinates\n",
    "# These source and destination points are defined to warp the image\n",
    "# to a grid where each 10x10 pixel square represents 1 square meter\n",
    "# The destination box will be 2*dst_size on each side\n",
    "dst_size = 5 \n",
    "# Set a bottom offset to account for the fact that the bottom of the image \n",
    "# is not the position of the rover but a bit in front of it\n",
    "# this is just a rough guess, feel free to change it!\n",
    "bottom_offset = 6\n",
    "source = np.float32([[14, 140], [301 ,140],[200, 96], [118, 96]])\n",
    "destination = np.float32([[image.shape[1]/2 - dst_size, image.shape[0] - bottom_offset],\n",
    "                  [image.shape[1]/2 + dst_size, image.shape[0] - bottom_offset],\n",
    "                  [image.shape[1]/2 + dst_size, image.shape[0] - 2*dst_size - bottom_offset], \n",
    "                  [image.shape[1]/2 - dst_size, image.shape[0] - 2*dst_size - bottom_offset],\n",
    "                  ])\n",
    "warped = perspect_transform(grid_img, source, destination)\n",
    "plt.imshow(warped)\n",
    "#scipy.misc.imsave('../output/warped_example.jpg', warped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color Thresholding\n",
    "Define the color thresholding function from the lesson and apply it to the warped image\n",
    "\n",
    "**TODO:** Ultimately, you want your map to not just include navigable terrain but also obstacles and the positions of the rock samples you're searching for.  Modify this function or write a new function that returns the pixel locations of obstacles (areas below the threshold) and rock samples (yellow rocks in calibration images), such that you can map these areas into world coordinates as well.  \n",
    "**Hints and Suggestion:** \n",
    "* For obstacles you can just invert your color selection that you used to detect ground pixels, i.e., if you've decided that everything above the threshold is navigable terrain, then everthing below the threshold must be an obstacle!\n",
    "\n",
    "\n",
    "* For rocks, think about imposing a lower and upper boundary in your color selection to be more specific about choosing colors.  You can investigate the colors of the rocks (the RGB pixel values) in an interactive matplotlib window to get a feel for the appropriate threshold range (keep in mind you may want different ranges for each of R, G and B!).  Feel free to get creative and even bring in functions from other libraries.  Here's an example of [color selection](http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_colorspaces/py_colorspaces.html) using OpenCV.  \n",
    "\n",
    "* **Beware However:** if you start manipulating images with OpenCV, keep in mind that it defaults to `BGR` instead of `RGB` color space when reading/writing images, so things can get confusing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find out color threshold for rocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "num_samples = 6\n",
    "rock_fns = ['../calibration_images/rock%d_pixels.png' % i for i in range(1, num_samples + 1)]\n",
    "rock_imgs = [misc.imread(fn) for fn in rock_fns]\n",
    "for imgfile in rock_fns:\n",
    "    display(Image(filename=imgfile)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the min/max for r, g, b values\n",
    "rock_r = np.concatenate([rock_imgs[i][:,:,0].flatten() for i in range(num_samples)])\n",
    "rock_g = np.concatenate([rock_imgs[i][:,:,1].flatten() for i in range(num_samples)])\n",
    "rock_b = np.concatenate([rock_imgs[i][:,:,2].flatten() for i in range(num_samples)])\n",
    "\n",
    "rock_thresh_lo = (min(rock_r), min(rock_g), min(rock_b))\n",
    "rock_thresh_hi = (max(rock_r), max(rock_g), max(rock_b)) \n",
    "\n",
    "rock_thresh_lo, rock_thresh_hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify pixels above and below given thresholds\n",
    "def color_thresh(img, lo=(0, 0, 0), hi=(255, 255, 255)):\n",
    "    color_select = np.zeros_like(img[:,:,0])\n",
    "    mask = (img[:,:,0] >= lo[0]) & (img[:,:,1] >= lo[1]) & (img[:,:,2] >= lo[2]) & \\\n",
    "           (img[:,:,0] <= hi[0]) & (img[:,:,1] <= hi[1]) & (img[:,:,2] <= hi[2])\n",
    "    color_select[mask] = 1\n",
    "    return color_select\n",
    "\n",
    "navigable_thresh_lo = (170, 170, 170)\n",
    "\n",
    "# note: in some cases there will be pixels which are neither navigable,\n",
    "# nor obstacle by this definition, but that's not a big issue\n",
    "thresh_navigable = color_thresh(grid_img, lo=navigable_thresh_lo)\n",
    "thresh_obstacle = color_thresh(grid_img, hi=navigable_thresh_lo)\n",
    "thresh_rock = color_thresh(rock_img, lo=rock_thresh_lo, hi=rock_thresh_hi)\n",
    "\n",
    "map_navigable = perspect_transform(thresh_navigable, source, destination)\n",
    "map_obstacle = perspect_transform(thresh_obstacle, source, destination)\n",
    "map_rock = perspect_transform(thresh_rock, source, destination)\n",
    "\n",
    "# Plot navigable terrain, obstacles, and rocks:\n",
    "# - row 1 - original images\n",
    "# - row 2 - thresholded images\n",
    "# - row 3 - perpective transform to map\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.subplot(331)\n",
    "plt.imshow(grid_img)\n",
    "plt.subplot(332)\n",
    "plt.imshow(grid_img)\n",
    "plt.subplot(333)\n",
    "plt.imshow(rock_img)\n",
    "plt.subplot(334)\n",
    "plt.imshow(thresh_navigable, cmap='gray')\n",
    "plt.subplot(335)\n",
    "plt.imshow(thresh_obstacle, cmap='gray')\n",
    "plt.subplot(336)\n",
    "plt.imshow(thresh_rock, cmap='gray')\n",
    "plt.subplot(337)\n",
    "plt.imshow(map_navigable, cmap='gray')\n",
    "plt.subplot(338)\n",
    "plt.imshow(map_obstacle, cmap='gray')\n",
    "plt.subplot(339)\n",
    "plt.imshow(map_rock, cmap='gray')\n",
    "\n",
    "#scipy.misc.imsave('../output/warped_threshed.jpg', threshed*255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinate Transformations\n",
    "Define the functions used to do coordinate transforms and apply them to an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "# Define a function to convert from image coords to rover coords\n",
    "def rover_coords(binary_img):\n",
    "    # Identify nonzero pixels\n",
    "    ypos, xpos = binary_img.nonzero()\n",
    "    # Calculate pixel positions with reference to the rover position being at the \n",
    "    # center bottom of the image.  \n",
    "    x_pixel = -(ypos - binary_img.shape[0]).astype(np.float)\n",
    "    y_pixel = -(xpos - binary_img.shape[1]/2 ).astype(np.float)\n",
    "    return x_pixel, y_pixel\n",
    "\n",
    "# The inverse of rover_coords\n",
    "def img_coords(img, x_pixel, y_pixel):\n",
    "    ypos = (img.shape[0] - x_pixel).astype(int)\n",
    "    xpos = (img.shape[1]/2 - y_pixel).astype(int)\n",
    "    return xpos, ypos\n",
    "\n",
    "# Define a function to convert to radial coords in rover space\n",
    "def to_polar_coords(x_pixel, y_pixel):\n",
    "    # Convert (x_pixel, y_pixel) to (distance, angle) \n",
    "    # in polar coordinates in rover space\n",
    "    # Calculate distance to each pixel\n",
    "    dist = np.sqrt(x_pixel**2 + y_pixel**2)\n",
    "    # Calculate angle away from vertical for each pixel\n",
    "    angles = np.arctan2(y_pixel, x_pixel)\n",
    "    return dist, angles\n",
    "\n",
    "# Define a function to map rover space pixels to world space\n",
    "def rotate_pix(xpix, ypix, yaw):\n",
    "    # Convert yaw to radians\n",
    "    yaw_rad = yaw * np.pi / 180\n",
    "    xpix_rotated = (xpix * np.cos(yaw_rad)) - (ypix * np.sin(yaw_rad))\n",
    "                            \n",
    "    ypix_rotated = (xpix * np.sin(yaw_rad)) + (ypix * np.cos(yaw_rad))\n",
    "    # Return the result  \n",
    "    return xpix_rotated, ypix_rotated\n",
    "\n",
    "def translate_pix(xpix_rot, ypix_rot, xpos, ypos, scale): \n",
    "    # Apply a scaling and a translation\n",
    "    xpix_translated = (xpix_rot / scale) + xpos\n",
    "    ypix_translated = (ypix_rot / scale) + ypos\n",
    "    # Return the result  \n",
    "    return xpix_translated, ypix_translated\n",
    "\n",
    "# Define a function to apply rotation and translation (and clipping)\n",
    "# Once you define the two functions above this function should work\n",
    "def pix_to_world(xpix, ypix, xpos, ypos, yaw, world_size, scale):\n",
    "    # Apply rotation\n",
    "    xpix_rot, ypix_rot = rotate_pix(xpix, ypix, yaw)\n",
    "    # Apply translation\n",
    "    xpix_tran, ypix_tran = translate_pix(xpix_rot, ypix_rot, xpos, ypos, scale)\n",
    "    # Perform rotation, translation and clipping all at once\n",
    "    x_pix_world = np.clip(xpix_tran.astype(int), 0, world_size - 1)\n",
    "    y_pix_world = np.clip(ypix_tran.astype(int), 0, world_size - 1)\n",
    "    # Return the result\n",
    "    return x_pix_world, y_pix_world\n",
    "\n",
    "# Grab another random image\n",
    "idx = np.random.randint(0, len(img_list)-1)\n",
    "image = mpimg.imread(img_list[idx])\n",
    "warped = perspect_transform(image, source, destination)\n",
    "threshed = color_thresh(warped, lo=navigable_thresh_lo)\n",
    "\n",
    "# Calculate pixel values in rover-centric coords and distance/angle to all pixels\n",
    "xpix, ypix = rover_coords(threshed)\n",
    "dist, angles = to_polar_coords(xpix, ypix)\n",
    "mean_dir = np.mean(angles)\n",
    "\n",
    "# Do some plotting\n",
    "fig = plt.figure(figsize=(12,9))\n",
    "plt.subplot(221)\n",
    "plt.imshow(image)\n",
    "plt.subplot(222)\n",
    "plt.imshow(warped)\n",
    "plt.subplot(223)\n",
    "plt.imshow(threshed, cmap='gray')\n",
    "plt.subplot(224)\n",
    "plt.plot(xpix, ypix, '.')\n",
    "plt.ylim(-160, 160)\n",
    "plt.xlim(0, 160)\n",
    "arrow_length = 100\n",
    "x_arrow = arrow_length * np.cos(mean_dir)\n",
    "y_arrow = arrow_length * np.sin(mean_dir)\n",
    "plt.arrow(0, 0, x_arrow, y_arrow, color='red', zorder=2, head_width=10, width=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in saved data and ground truth map of the world\n",
    "The next cell is all setup to read your saved data into a `pandas` dataframe.  Here you'll also read in a \"ground truth\" map of the world, where white pixels (pixel value = 1) represent navigable terrain.  \n",
    "\n",
    "After that, we'll define a class to store telemetry data and pathnames to images.  When you instantiate this class (`data = Databucket()`) you'll have a global variable called `data` that you can refer to for telemetry and map data within the `process_image()` function in the following cell.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Import pandas and read in csv file as a dataframe\n",
    "import pandas as pd\n",
    "# Change the path below to your data directory\n",
    "# If you are in a locale (e.g., Europe) that uses ',' as the decimal separator\n",
    "# change the '.' to ','\n",
    "df = pd.read_csv('../test_dataset/robot_log.csv', delimiter=';', decimal='.')\n",
    "csv_img_list = df[\"Path\"].tolist() # Create list of image pathnames\n",
    "# Read in ground truth map and create a 3-channel image with it\n",
    "ground_truth = mpimg.imread('../calibration_images/map_bw.png')\n",
    "ground_truth_3d = np.dstack((ground_truth*0, ground_truth*255, ground_truth*0)).astype(np.float)\n",
    "\n",
    "# Creating a class to be the data container\n",
    "# Will read in saved data from csv file and populate this object\n",
    "# Worldmap is instantiated as 200 x 200 grids corresponding \n",
    "# to a 200m x 200m space (same size as the ground truth map: 200 x 200 pixels)\n",
    "# This encompasses the full range of output position values in x and y from the sim\n",
    "class Databucket():\n",
    "    def __init__(self):\n",
    "        self.images = csv_img_list  \n",
    "        self.xpos = df[\"X_Position\"].values\n",
    "        self.ypos = df[\"Y_Position\"].values\n",
    "        self.yaw = df[\"Yaw\"].values\n",
    "        self.count = 0 # This will be a running index\n",
    "        self.worldmap = np.zeros((200, 200, 3)).astype(np.float)\n",
    "        self.navigable = np.zeros((200,200)).astype(int)\n",
    "        self.explored = np.zeros((200,200)).astype(int)\n",
    "        self.unexplored = np.zeros((200,200)).astype(int)\n",
    "        self.ground_truth = ground_truth_3d # Ground truth worldmap\n",
    "\n",
    "# Instantiate a Databucket().. this will be a global variable/object\n",
    "# that you can refer to in the process_image() function below\n",
    "data = Databucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a function to process stored images\n",
    "\n",
    "Modify the `process_image()` function below by adding in the perception step processes (functions defined above) to perform image analysis and mapping.  The following cell is all set up to use this `process_image()` function in conjunction with the `moviepy` video processing package to create a video from the images you saved taking data in the simulator.  \n",
    "\n",
    "In short, you will be passing individual images into `process_image()` and building up an image called `output_image` that will be stored as one frame of video.  You can make a mosaic of the various steps of your analysis process and add text as you like (example provided below).  \n",
    "\n",
    "\n",
    "\n",
    "To start with, you can simply run the next three cells to see what happens, but then go ahead and modify them such that the output video demonstrates your mapping process.  Feel free to get creative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions to create a custom mosaic arrangement from several images\n",
    "\n",
    "# Find out where to put images, first columns then row, so they don't overlap\n",
    "def image_positions(num_columns, *imgs):\n",
    "    next_y = 0 \n",
    "    for i in range(len(imgs)):\n",
    "        (x, y) = (x + imgs[i - 1].shape[1], y) if i % num_columns else (0, next_y)\n",
    "        next_y = max(next_y, y + imgs[i].shape[0])\n",
    "        yield (y, x)\n",
    "\n",
    "# Put images in the layout defined by the number of columns\n",
    "def arrange_images(num_columns, *imgs):\n",
    "    pos = list(image_positions(num_columns, *imgs))\n",
    "    size = lambda dim: max(pos[i][dim] + imgs[i].shape[dim] for i in range(len(imgs)))\n",
    "    output_image = np.zeros((size(0), size(1), 3))\n",
    "    for i in range(len(imgs)):\n",
    "        output_image[pos[i][0]:pos[i][0] + imgs[i].shape[0],\n",
    "                     pos[i][1]:pos[i][1] + imgs[i].shape[1]] = imgs[i]\n",
    "\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Misc utility functions\n",
    "def cartesian_product(*arrays):\n",
    "    la = len(arrays)\n",
    "    dtype = np.result_type(*arrays)\n",
    "    arr = np.empty([len(a) for a in arrays] + [la], dtype=dtype)\n",
    "    for i, a in enumerate(np.ix_(*arrays)):\n",
    "        arr[...,i] = a\n",
    "    return arr.reshape(-1, la)\n",
    "\n",
    "# return all the coordinate points within an area with a given granularity\n",
    "def get_coords(area, gran):\n",
    "    xs = np.arange(area[0][0], area[1][0], gran, dtype=float)\n",
    "    ys = np.arange(area[0][1], area[1][1], gran, dtype=float)\n",
    "    coords = np.transpose(cartesian_product(xs, ys))\n",
    "    return coords[0], coords[1]\n",
    "    \n",
    "#get_coords(((-2, -2), (2, 2)), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper class to encompass information about the rover and the world\n",
    "class Perspective:\n",
    "    def __init__(self, xpos, ypos, yaw, world_size, scale, source, destination):\n",
    "        # position and orientation of the rover\n",
    "        self.xpos = xpos\n",
    "        self.ypos = ypos\n",
    "        self.yaw = yaw\n",
    "        # needed for transforming to world coordinates\n",
    "        self.world_size = world_size\n",
    "        self.scale = scale\n",
    "        # needed for perspective transform\n",
    "        self.source = source\n",
    "        self.destination = destination\n",
    "        # don't trust camera pixels too close to the horizon\n",
    "        self.trust_area = ((0.0, -70.0), (70.0, 70))\n",
    "        # we will use this enlarged box to keep track of unexplored areas \n",
    "        self.mask_area = ((-10.0, -90.0), (80.0, 90))\n",
    "        \n",
    "# Helper class to calculate information about a subset of the terrain\n",
    "class TerrainSet():\n",
    "    def __init__(self, persp, camera_thresh_img):\n",
    "        # apply transformation to rover coordinates\n",
    "        self.threshed = camera_thresh_img\n",
    "        self.x_rover, self.y_rover = rover_coords(perspect_transform(self.threshed, persp.source, persp.destination))\n",
    "        # clip points far away close to the horizon (they're too far anyway and they mess up fidelity)\n",
    "        self.x_rover = np.clip(self.x_rover.astype(int), persp.trust_area[0][0], persp.trust_area[1][0])\n",
    "        self.y_rover = np.clip(self.y_rover.astype(int), persp.trust_area[0][1], persp.trust_area[1][1])\n",
    "        self.x_img, self.y_img = img_coords(camera_thresh_img, self.x_rover, self.y_rover)\n",
    "        self.warped = np.zeros_like(camera_thresh_img)\n",
    "        self.warped[self.y_img, self.x_img] = 1\n",
    "        self.x_world, self.y_world = pix_to_world(self.x_rover, self.y_rover,\n",
    "                                                  persp.xpos, persp.ypos, persp.yaw, persp.world_size, persp.scale)\n",
    "        \n",
    "# Helper class to calculate a mask over a given area relative to the rover\n",
    "class TerrainMask():\n",
    "    def __init__(self, persp, rover_area):\n",
    "        self.x_rover, self.y_rover = get_coords(rover_area, 1.0)\n",
    "        self.x_world, self.y_world = pix_to_world(self.x_rover, self.y_rover,\n",
    "                                                  persp.xpos, persp.ypos, persp.yaw,\n",
    "                                                  persp.world_size, persp.scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to pass stored images to\n",
    "# reading rover position and yaw angle from csv file\n",
    "# This function will be used by moviepy to create an output video\n",
    "def process_image(img):\n",
    "\n",
    "    # Define source and destination for perspective transform\n",
    "    dst_size = 5 \n",
    "    bottom_offset = 6\n",
    "    src = np.float32([[14, 140], [301 ,140],[200, 96], [118, 96]])\n",
    "    dst = np.float32([[img.shape[1]/2 - dst_size, img.shape[0] - bottom_offset],\n",
    "                              [img.shape[1]/2 + dst_size, img.shape[0] - bottom_offset],\n",
    "                              [img.shape[1]/2 + dst_size, img.shape[0] - 2*dst_size - bottom_offset], \n",
    "                              [img.shape[1]/2 - dst_size, img.shape[0] - 2*dst_size - bottom_offset]])\n",
    "    \n",
    "    persp = Perspective(xpos = data.xpos[data.count],\n",
    "                        ypos = data.ypos[data.count],\n",
    "                        yaw = data.yaw[data.count],\n",
    "                        world_size = data.worldmap.shape[0],\n",
    "                        scale = 2 * dst_size,\n",
    "                        source = src, destination = dst)\n",
    "    \n",
    "    # Navigable terrain \n",
    "    nav_terrain = TerrainSet(persp, color_thresh(img, lo=navigable_thresh_lo))\n",
    "    \n",
    "    # Obstacles \n",
    "    obs_terrain = TerrainSet(persp, color_thresh(img, hi=navigable_thresh_lo))\n",
    "\n",
    "    # Rock samples\n",
    "    rock_terrain = TerrainSet(persp, color_thresh(img, lo=rock_thresh_lo, hi=rock_thresh_hi))   \n",
    "    \n",
    "    # Update world map and overlay ground truth map\n",
    "    data.worldmap[obs_terrain.y_world, obs_terrain.x_world, 0] = 255\n",
    "    data.worldmap[rock_terrain.y_world, rock_terrain.x_world, 1] = 255\n",
    "    data.worldmap[nav_terrain.y_world, nav_terrain.x_world, 2] = 255\n",
    "    # Resolve conflicts: prefer navigable, when it is both navigable and obstacle\n",
    "    data.worldmap[data.worldmap[:, :, 2].nonzero(), 0] = 0\n",
    "    \n",
    "    # This is used for pathfinding\n",
    "    data.navigable[nav_terrain.y_world, nav_terrain.x_world] += 4\n",
    "    data.navigable[obs_terrain.y_world, obs_terrain.x_world] -= 1\n",
    "    \n",
    "    map_overlay = np.flipud(cv2.addWeighted(data.worldmap, 1, data.ground_truth, 1, 0))\n",
    "     \n",
    "    # Calculate unexplored areas\n",
    "    explored = TerrainSet(persp, np.ones_like(img[:,:,0]))\n",
    "    mask = TerrainMask(persp, persp.mask_area)\n",
    "    \n",
    "    data.explored[explored.y_world, explored.x_world] = 1\n",
    "    data.unexplored[mask.y_world, mask.x_world] = 1\n",
    "    data.unexplored = (data.unexplored > 0) & (data.explored == 0)\n",
    "    \n",
    "    # Map constructed by rover (including unexplored areas)\n",
    "    rover_map = np.zeros_like(data.worldmap).astype(np.float)\n",
    "    rover_map[:, :, 0] = data.worldmap[:, :, 0]\n",
    "    rover_map[:, :, 1] = data.worldmap[:, :, 2]\n",
    "    rover_map[:, :, 2] = data.unexplored * 255\n",
    "        \n",
    "    # Thresholded image from rover POV\n",
    "    thresh = np.concatenate([obs_terrain.threshed[:,:,np.newaxis]*128,\n",
    "                             rock_terrain.threshed[:,:,np.newaxis]*255,\n",
    "                             nav_terrain.threshed[:,:,np.newaxis]*128], axis=2)\n",
    "    \n",
    "    # Thresholded image from above\n",
    "    warped = np.concatenate([obs_terrain.warped[:,:,np.newaxis]*128,\n",
    "                             rock_terrain.warped[:,:,np.newaxis]*255,\n",
    "                             nav_terrain.warped[:,:,np.newaxis]*128], axis=2)\n",
    "    \n",
    "    # Combine images\n",
    "    output_image = arrange_images(2, img, warped, map_overlay, np.flipud(rover_map))\n",
    "\n",
    "    if data.count < len(data.images) - 1:\n",
    "        data.count += 1 # Keep track of the index in the Databucket()\n",
    "    \n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test how it looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "fn = '../output/test_image.jpg'\n",
    "data = Databucket()\n",
    "data.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = mpimg.imread(img_list[data.count])\n",
    "test_image = process_image(image)\n",
    "data.count = data.count + 1\n",
    "misc.imsave(fn, test_image)\n",
    "Image(filename=fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a video from processed image data\n",
    "Use the [moviepy](https://zulko.github.io/moviepy/) library to process images and create a video.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "\n",
    "# Define pathname to save the output video\n",
    "output = '../output/test_mapping.mp4'\n",
    "data = Databucket() # Re-initialize data in case you're running this cell multiple times\n",
    "clip = ImageSequenceClip(data.images, fps=60) # Note: output video will be sped up because \n",
    "                                          # recording rate in simulator is fps=25\n",
    "new_clip = clip.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "%time new_clip.write_videofile(output, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This next cell should function as an inline video player\n",
    "If this fails to render the video, try running the following cell (alternative video rendering method).  You can also simply have a look at the saved mp4 in your `/output` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is an alternative way to create a video in case the above cell did not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "video = io.open(output, 'r+b').read()\n",
    "encoded_video = base64.b64encode(video)\n",
    "HTML(data='''<video alt=\"test\" controls>\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded_video.decode('ascii')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test shortest path calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_path(points, src, dst, predecessors):\n",
    "    path = []\n",
    "    idx = dst\n",
    "    while (idx != src) and (idx > 0):\n",
    "        path.insert(0, points[idx])\n",
    "        idx = predecessors[idx]\n",
    "    if (idx > 0):\n",
    "        path.insert(0, points[idx])\n",
    "        return path\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import dijkstra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the shortest path from an explored point to the closest unexplored point\n",
    "navigable_points = np.transpose(np.nonzero(data.navigable > 0))\n",
    "unexplored_points = np.transpose(data.unexplored.nonzero())\n",
    "all_points = np.concatenate((navigable_points, unexplored_points))\n",
    "\n",
    "src = np.argwhere(np.all((all_points-np.array([191,103])==0), axis=1)).flatten()[0]\n",
    "#src = np.argwhere(np.all((all_points-np.array([85,100])==0), axis=1)).flatten()[0]\n",
    "\n",
    "dist_matrix = squareform(pdist(all_points, metric='cityblock') < 1.5)\n",
    "unexplored_idx = range(len(navigable_points), len(all_points))\n",
    "dist_matrix[unexplored_idx, unexplored_idx] = False\n",
    "\n",
    "graph = csr_matrix(dist_matrix)\n",
    "distances, predecessors = dijkstra(graph, directed=False, indices=src, return_predecessors=True)\n",
    "\n",
    "dst_2 = np.argmin(distances[unexplored_idx])\n",
    "dst = unexplored_idx[dst_2]\n",
    "\n",
    "path = get_path(all_points, src, dst, predecessors)\n",
    "# print path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = '../output/dijkstra.png'\n",
    "\n",
    "y_path, x_path = np.transpose(path)\n",
    "\n",
    "map = np.zeros_like(ground_truth_3d)\n",
    "map[y_path, x_path, 0] = 255\n",
    "map[:,:,1] = (data.navigable > 0) * 255\n",
    "map[:,:,2] = data.unexplored * 255\n",
    "map[y_path, x_path, 1] = 0\n",
    "\n",
    "misc.imsave(fn, map)\n",
    "Image(filename=fn, width=600, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the closest point to a given point\n",
    "def closest_point_idx(pointset, p):\n",
    "    return np.argmin(np.sum((pointset - p)**2, axis=1))\n",
    "\n",
    "# return points closer than a given distance\n",
    "def neighbourhood(pointset, p, d):\n",
    "    dist_2, d_2 = np.sum((pointset - p)**2, axis=1), d**2\n",
    "    return np.nonzero(dist_2 < d_2)\n",
    "    \n",
    "#print closest_point_idx(path, np.array([100, 100]))\n",
    "#print neighbourhood(path, np.array([100, 100]), 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1.1, 2.1, 3.1], dtype=float)\n",
    "print np.argmin(abs(a-4.0), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.nonzero(data.unexplored > 0)\n",
    "np.transpose(data.unexplored.nonzero()) + [0.5, 0.5]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
